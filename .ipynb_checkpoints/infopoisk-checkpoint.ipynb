{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "#### Факт 1.\n",
    "Главной героине первого советского телесериала присвоили звание Героя Советского Союза по настоянию зрителей\n",
    "\n",
    "\"После показа этого фильма ветераны Великой Отечественной войны, общественные организации обратились к руководству СССР с предложением присвоить Анне Морозовой звание Героя Советского Союза.\"\n",
    "\n",
    "#### Факт 2.\n",
    "«Выхино» — самая загруженная станция Московского метрополитена: согласно измерениям 2002 года, суточный пассажиропоток на станции составлял 174 250 человек.\n",
    "\n",
    "\"Является одной из самых загруженных станций московского метро\"\n",
    "Странно то, что ни в одной ссылке точных чисел (174 250) не было.\n",
    "\n",
    "####  Факт 3.\n",
    "Официальное действие старых советских паспортов на территории России было прекращено 1 июля 2004 года, а, например, в Приднестровье советские паспорта до сих пор являются законным документом, удостоверяющим личность\n",
    "\n",
    "\"С распадом СССР в Российской Федерации Министерством внутренних дел Российской Федерации была предпринята попытка осуществить обмен паспортов гражданина СССР в срок до 1 июля 2004 года.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "1. Установить морфологический анализатор\n",
    "* Выбрать текст\n",
    "* Сделать частотный список лемм\n",
    "* Выдать по мере снижения частоты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "def tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    text_list = nltk.word_tokenize(sentence)\n",
    "    text_list = [word for word in text_list if len(word) > 1]\n",
    "    text_list = [morph.parse(word)[0].normal_form for word in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'data/task2.txt'\n",
    "with open(filename, 'r') as fin:\n",
    "    file_text = fin.read()\n",
    "print ('Первые 1000 символов:')\n",
    "print (file_text[:1000])\n",
    "print ('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "file_text = file_text.replace('.', ' ')\n",
    "lemms = np.array(tokenize(file_text))\n",
    "values, count = np.unique(lemms, return_counts=True)\n",
    "unique_values = [(x,y) for x,y in zip(values, count)]\n",
    "unique_values.sort(key=lambda x: x[1], reverse=True)\n",
    "for i in unique_values[:15]:\n",
    "    print ('{1}\\t{0}'.format(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3.1\n",
    "![title](./data/img/task3_1.png)\n",
    "Запрос - \"сar insurance\"\n",
    "1. Вычислить вес каждого документа\n",
    "2. Представить запрос как вектор\n",
    "3. Представить документ как вектор\n",
    "4. Вычислить сходство  запроса и документа\n",
    "\n",
    "Вектор документа нормализуется, вектор запроса не нормализуется\n",
    "Показать, какие веса у документов по отношению к запросу и как упорядочатся документы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br \\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = np.array([1.65, 2.08, 1.62, 1.5])\n",
    "query = np.array([1, 0, 1, 0]) * idf\n",
    "d1 = normalize((np.array([27, 3, 0, 14]) * idf).reshape(1,-1))[0]\n",
    "d2 = normalize((np.array([4, 33, 33, 0]) * idf).reshape(1,-1))[0]\n",
    "d3 = normalize((np.array([24, 0, 29, 17]) * idf).reshape(1,-1))[0]\n",
    "print ('query:\\t{}'.format(query))\n",
    "print ('d1:\\t{}'.format(d1))\n",
    "print ('d2:\\t{}'.format(d2))\n",
    "print ('d3:\\t{}'.format(d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = {'d1': d1, 'd2': d2, 'd3': d3}\n",
    "print ('cosine similarity')\n",
    "for i in sorted(docs.items(), key=lambda x:cosine_similarity(x[1].reshape(1,-1), query.reshape(1,-1)), reverse=True):\n",
    "    print ('{0} : {1}'.format(i[0], cosine_similarity(i[1].reshape(1,-1), query.reshape(1,-1))[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 3.2\n",
    "\n",
    "Запросы – это проанализированные вами факты из Википедии.\n",
    "<br />\n",
    "Документы – это предложения из статей Википедии, указанных в этих фактах\n",
    "<br />\n",
    "Все должно быть обработано морфологическим анализатором\n",
    "<br />\n",
    "Нужно найти наиболее релевантные предложения \n",
    "1. По векторной модели без idf\n",
    "2. По tf.idf (idf в данном случае – это количество предложений, в которых встречалось  слово)\n",
    "\n",
    "\n",
    "Нормализация запроса и предложения, т.е. выстроить все предложения из статей по мере сходства с запросом по векторной модели. <br\\>\n",
    "В отчете должны быть показаны веса выдаваемых предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_parse(doc_name):\n",
    "    with open(doc_name, 'r') as fin:\n",
    "        file = fin.read()\n",
    "    file = file.replace('\\n', '')\n",
    "    sentences = file.split('.')\n",
    "    query, answer = None, None\n",
    "    if sentences[0][0] == '*':\n",
    "        query = sentences[0][1:]\n",
    "    if sentences[1][0] == '#':\n",
    "        answer = sentences[1][1:]\n",
    "    return query, answer, sentences[2:]\n",
    "def tf_idf(query, sentences, use_idf=False):\n",
    "    tfs = TfidfVectorizer(tokenizer=tokenize, use_idf=use_idf)\n",
    "    sentences.append(query)\n",
    "    docs_vec = tfs.fit_transform(sentences)\n",
    "    query_vec = docs_vec[-1]\n",
    "    docs_vec = docs_vec[:-1]\n",
    "    return query_vec, docs_vec\n",
    "def print_all(filename, use_idf=False):\n",
    "    query, answer, sentences = doc_parse(filename)\n",
    "    query_vec, sentences_vec = tf_idf(query, sentences, use_idf)\n",
    "    sentences_cos_similarity = [cosine_similarity(query_vec, x)[0][0] for x in sentences_vec]\n",
    "    common_list = [(k,v) for k,v in zip(sentences,sentences_cos_similarity)]\n",
    "    common_list = sorted(common_list, key=lambda x:x[1], reverse=True)\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    if use_idf:\n",
    "        print ('~~~~~~~~with idf~~~~~~~~~~')\n",
    "    else:\n",
    "        print ('~~~~~~without idf~~~~~~~~~')\n",
    "    print ('Query: {}'.format(query))\n",
    "    print ('Answer: {}'.format(answer))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    for i in common_list[:5]:\n",
    "        print ('{0:.5f} {1}'.format(i[1], i[0]))\n",
    "    print ('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = ['data/task3/facts.txt', 'data/task3/facts2.txt','data/task3/facts3.txt']\n",
    "for file in files:\n",
    "    print_all(file, use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = ['data/task3/facts.txt', 'data/task3/facts2.txt','data/task3/facts3.txt']\n",
    "for file in files:\n",
    "    print_all(file, use_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 4.1\n",
    "Эксперт нашел 20 релевантных документов. Система нашла 4 документа в следующей последовательности релевантных и нерелевантных документов: \n",
    "* RNRNNRRNNNN\n",
    "\n",
    "Какова средняя  точность поиска – Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = [1/1, 2/3, 3/6, 4/7]\n",
    "all = 20\n",
    "print ('average precision: {}'.format(sum(R) / all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 4.2\n",
    "При разметке релевантных документов эксперты использовали шкалу от 0 (нерелевантные документы) до 4 баллов.\n",
    "\n",
    "При тестировании систем выяснилось, что системы выдали следующие результаты поиска ответов на запрос:\n",
    "* Система 1: 4, 2, 3, 1, 2, 0 (и далее 0)\n",
    "* Система 2: 3, 2, 4, 4, 4., далее 0.\n",
    "\n",
    "Какая система ищет лучше по мере NDCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dcg(vec):\n",
    "    dcg = vec[0]\n",
    "    for i in range(1, len(vec)):\n",
    "        dcg += vec[i] / math.log(i+1)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "число idcg будет одинаковым, по-этому сравним лишь dcg для каждой системы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "system_1 = [4, 2, 4, 1, 2]\n",
    "system_2 = [3, 2, 4, 4, 4]\n",
    "dcg1 = dcg(system_1)\n",
    "dcg2 = dcg(system_2)\n",
    "print('system 1: {}'.format(dcg(system_1)))\n",
    "print('system 2: {}'.format(dcg(system_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: Система 2 лучше по мере NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 5.1\n",
    "Запрос: *отбор кандидатов*\n",
    "Пользователь отметил релевантными два документа\n",
    "* *Кандидат отобрать претендент*\n",
    "* *Отбор выбрать претендент*\n",
    "\n",
    "Объем коллекции – 1 млн.документов\n",
    "Df: \n",
    "* отбор -      70000\n",
    "* кандидат -   70000\n",
    "* Претендент - 30000\n",
    "* отобрать -   50000\n",
    "* выбрать -    70000\n",
    "\n",
    "Как изменится запрос, если\n",
    "* alpha=0.7 (коэффициент учета запроса) \n",
    "* beta=0.3 (коэффициент учета релевантных документов)\n",
    "\n",
    "Запрос представляется как вектор частот\n",
    "<br \\>\n",
    "Документ представляется как нормализованный вектор tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем вектора запроса и документов:\n",
    "\n",
    "|          |отбор|кандидат|претендент|отобрать|выбрать\n",
    "|----------|-----|--------|----------|--------|-------|\n",
    "|запрос\t   |  1  |   1    |    0     |   0    |   0   |\n",
    "|документ 1|  0  |   1    |    1     |   1    |   0   |\n",
    "|документ 2|  1  |   0    |    1     |   0    |   1   |\n",
    "\n",
    "Посчитаем idf для каждого слова:\n",
    "\n",
    "* $idf_{отбор}$ = 1.15\n",
    "* $idf_{кандидат}$ = 1.15\n",
    "* $idf_{претендент}$ = 1.52\n",
    "* $idf_{отобрать}$ = 1.3\n",
    "* $idf_{выбрать}$ = 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf_vector = np.array([1.15, 1.15,1.52, 1.3, 1.15])\n",
    "query = np.array([1,1,0,0,0])\n",
    "doc1 = np.array([0,1,1,1,0])\n",
    "doc2 = np.array([1,0,1,0,1])\n",
    "\n",
    "print ('docs tf-idf')\n",
    "print ('doc1: {}'.format(doc1 * idf_vector))\n",
    "print ('doc2: {}'.format(doc2 * idf_vector))\n",
    "\n",
    "doc1 = normalize((doc1 * idf_vector).reshape(1,-1))[0]\n",
    "doc2 = normalize((doc2 * idf_vector).reshape(1,-1))[0]\n",
    "\n",
    "print ('~~~~~~~~~~~~')\n",
    "print ('normalize docs')\n",
    "print ('doc1: {}'.format(doc1))\n",
    "print ('doc2: {}'.format(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем новый вектор запроса по формуле:\n",
    "$\\alpha * Запрос  + \\beta * \\frac{1}{2} * \\sum_{i=1}^{2}{документ_i}$\n",
    "<br \\>при \n",
    "* $\\alpha = 0.7$\n",
    "* $\\beta = 0.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('new_query: {}'.format(0.7 * query + 0.3 * 0.5 * doc1 + 0.3 * 0.5 * doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 5.2\n",
    "* Запросы по Википедии\n",
    "Для каждого запроса сделать идеальную разметку, т.е. разметить предложения, насколько они релевантны запросу\n",
    "* Использовать трехбалльную шкалу {2,1, 0}\n",
    "* Оценить качество выдачи, используя NDCG\n",
    "* Идеальное расположение предложений с оценками\n",
    "* Расчет NDCG для каждого запроса и варианта выдачи\n",
    "* Найти среднее для метода с idf и без idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IDCG_{facts.txt} = 2 + \\dfrac{2}{log(2)} + \\dfrac{1}{log(3)} + \\dfrac{1}{log(4)} = 6.517$\n",
    "<br\\>\n",
    "<br\\>\n",
    "$IDCG_{facts2.txt} = 2 + \\dfrac{1}{log(2)} + \\dfrac{1}{log(3)} + \\dfrac{1}{log(4)} = 5.074$\n",
    "<br \\>\n",
    "<br\\>\n",
    "$IDCG_{facts3.txt} = 2 = 2.000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDCG_dict = {'data/task5/facts.txt': 6.517, 'data/task5/facts2.txt': 5.074, 'data/task5/facts3.txt': 2.000}\n",
    "def doc_parse_ndcg(doc_name):\n",
    "    with open(doc_name, 'r') as fin:\n",
    "        file = fin.read()\n",
    "    file = file.replace('\\n', '')\n",
    "    sentences = file.split('.')\n",
    "    query, answer = None, None\n",
    "    if sentences[0][0] == '*':\n",
    "        query = sentences[0][1:]\n",
    "    new_sentences = []\n",
    "    for sentence in sentences[1:]:\n",
    "        r = 0\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) > 0 and sentence[0] == '(':\n",
    "            r = int(sentence[1])\n",
    "            sentence = sentence[3:]\n",
    "        new_sentences.append((sentence, r))\n",
    "    return query, new_sentences\n",
    "\n",
    "def tf_idf_ndcg(query, sentences, use_idf=False):\n",
    "    tfs = TfidfVectorizer(tokenizer=tokenize, use_idf=use_idf)\n",
    "    sentences.append((query,0))\n",
    "    docs_vec = tfs.fit_transform([x[0] for x in sentences])\n",
    "    query_vec = docs_vec[-1]\n",
    "    docs_vec = docs_vec[:-1]\n",
    "    return query_vec, [(x,y[1]) for x,y in zip(docs_vec,sentences)]\n",
    "\n",
    "def print_ndcg(filename, use_idf=False):\n",
    "    query, sentences = doc_parse_ndcg(filename)\n",
    "    query_vec, sentences_vec = tf_idf_ndcg(query, sentences, use_idf)\n",
    "    sentences_cos_similarity = [cosine_similarity(query_vec, x[0])[0][0] for x in sentences_vec]\n",
    "    common_list = [(k,v) for k,v in zip(sentences,sentences_cos_similarity)]\n",
    "    common_list = sorted(common_list, key=lambda x:x[1], reverse=True)\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    if use_idf:\n",
    "        print ('~~~~~~~~with idf~~~~~~~~~~')\n",
    "    else:\n",
    "        print ('~~~~~~without idf~~~~~~~~~')\n",
    "    print ('Query: {}'.format(query))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    ndcg = common_list[0][0][1]\n",
    "    for idx, i in enumerate(common_list[:10]):\n",
    "        if idx > 0:\n",
    "            ndcg += i[0][1] / math.log(idx+1)\n",
    "        print ('({2}) {0:.5f} {1}'.format(i[1], i[0][0],i[0][1]))\n",
    "    ndcg = ndcg / IDCG_dict[filename]\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~\\nNDCG = {}'.format(ndcg))\n",
    "    print ('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = ['data/task5/facts.txt', 'data/task5/facts2.txt','data/task5/facts3.txt']\n",
    "for file in files:\n",
    "    print_ndcg(file, use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = ['data/task5/facts.txt', 'data/task5/facts2.txt','data/task5/facts3.txt']\n",
    "for file in files:\n",
    "    print_ndcg(file, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NDCG_arr = np.array([0.6069561537803363, 0.5807628312469977, 0.228499744749743])\n",
    "NDCG_arr_idf = np.array([0.6069561537803363, 0.5464826396126844, 0.2543848247941093])\n",
    "print ('average ndcg without idf: {}'.format(NDCG_arr.mean()))\n",
    "print ('average ndcg with idf: {}'.format(NDCG_arr_idf.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 6\n",
    "Встретилось слово hodtel.\n",
    "<br \\>Какое исходно слово наиболее вероятно: hotel, hostel?\n",
    "* Ошибка od|o встречалась 9 раз\n",
    "* Ошибка d|s- встречалась 7 раз\n",
    "* Hotel встречается в пять раз чаще, чем hostel\n",
    "* Буква o встречается в корпусе в 1.2 раза чаще, чем буква s\n",
    "\n",
    "Два варианта расчета: \n",
    "1. Без сглаживания\n",
    "* Сглаживание по Лапласу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без сглаживания:\n",
    "\n",
    "$p_{hotel} = (9 * 5) / (n * 1.2) = 37.5 / n$\n",
    "<br\\>\n",
    "$p_{hostel} = (7 * 1) / (n * 1) = 7 / n$\n",
    "\n",
    "Наиболее вероятный: hotel\n",
    "\n",
    "Со сглаживанием по Лапласу:\n",
    "\n",
    "$p_{hotel} = ((9 + 1) * 5) / (n * 1.2 + 26) = 50 / (1.2n + 26)$\n",
    "<br\\>\n",
    "$p_{hostel} = ((7 + 1) * 1) / (n * 1 + 26) = 8 / (n + 26)$\n",
    "\n",
    "Наиболее вероятный: hotel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 7.1\n",
    "Даны документы и их классы С1 и С2\n",
    "* D1=(X1,X2, X3) - C1\n",
    "* D2=(X1, X2, X4) - C1\t\t\n",
    "* D3=(X4, X5, X6) - C2\t\n",
    "\n",
    "Определить класс документа на основе метода наивного Байеса \n",
    "<br\\>\n",
    "D4=(X1, X4, X5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод наивного Байеса:\n",
    "\n",
    "Вычисление вероятностей:\n",
    "\n",
    "$P(C1) = 2 / 3 = 0.67$\n",
    "<br\\>\n",
    "$P(X1 | C1) = (2 + 1) / 12 = 0.25$\n",
    "<br\\>\n",
    "$P(X4 | C1) = (1 + 1) / 12 = 0.17$\n",
    "<br\\>\n",
    "$P(X5 | C1) = (0 + 1) / 12 = 0.08$\n",
    "\n",
    "$P(C2) = 1 / 3 = 0.33$\n",
    "<br\\>\n",
    "$P(X1 | C2) = (0 + 1) / 9 = 0.11$\n",
    "<br\\>\n",
    "$P(X4 | C2) = (1 + 1) / 9 = 0.22$\n",
    "<br\\>\n",
    "$P(X5 | C2) = (1 + 1) / 9 = 0.22$\n",
    "\n",
    "$score_{C1} = 0.67 * 0.25 * 0.17 * 0.08 = 0.002278$\n",
    "<br\\>\n",
    "$score_{C2} = 0.33 * 0.11 * 0.22 * 0.22 = 0.001757$\n",
    "\n",
    "D4 принадлежит классу C1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 7.2\n",
    "* Зайти news.yandex.ru\n",
    "* Набрать 10 разных текстов из рубрики Политика\n",
    "* И 10 текстов из другой рубрики, например, Спорт (или другая)\n",
    "\n",
    "Итого – 20 текстов в двух рубриках\n",
    "<br\\>\n",
    "Система классификации методом Байеса\n",
    "<br\\>\n",
    "Можно задать текст и система будет классифицировать\n",
    "<br\\>\n",
    "Показывать полученные веса классов для текущего документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label2name = {0: 'politic', 1: 'sport'}\n",
    "tfs = TfidfVectorizer(tokenizer=tokenize)\n",
    "filepath = 'data/task7'\n",
    "news = []\n",
    "for idx in range(10):\n",
    "    file = filepath + '/politic/politic' + str(idx) + '.txt'\n",
    "    with open(file, 'r') as fin:\n",
    "        text = fin.read()\n",
    "    news.append(text)\n",
    "for idx in range(10):\n",
    "    file = filepath + '/sport/sport' + str(idx) + '.txt'\n",
    "    with open(file, 'r') as fin:\n",
    "        text = fin.read()\n",
    "    news.append(text)\n",
    "assert len(news) == 20, 'Error'\n",
    "X_train = tfs.fit_transform(news)\n",
    "y_train = np.append(np.zeros(10), np.ones(10))\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_politic_1 = 'Савченко заявила о договорённости с Донбассом по обмену всеми пленным. Народный депутат Украины Надежда Савченко заявила о достижении договорённости с руководством Донецкой и Луганской народных республик об обмене пленными по формуле \"4 на 1\".'\n",
    "test_politic_2 = 'Путин возглавил рейтинг Forbes самых влиятельных людей мира. Президент России Владимир Путин возглавил рейтинг самых влиятельных людей, составленный журналом Forbes. Он опубликован на сайте журнала.'\n",
    "test_sport_1 = 'Женская сборная России по гандболу завершила выступление на ЧЕ. Заключительная встреча с командой Венгрии, которая ничего не решала для россиянок, закончилась вничью — 26:26 Женская сборная России по гандболу сыграла вничью с командой Венгрии в последнем матче второго группового этапа чемпионата Европы, который проходит в Швеции.'\n",
    "test_sport_2 = 'Третий этап розыгрыша Кубка мира по биатлону стартует в четверг в чешском городе Нове- Место-на-Мораве. Впервые в сезоне пройдут гонки с масс- старта среди мужчин и женщин, участники также будут соревноваться в спринте и гонках преследования.'\n",
    "\n",
    "test_arr = [test_politic_1, test_politic_2, test_sport_1, test_sport_2]\n",
    "\n",
    "for test_sentence in test_arr:\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print (test_sentence)\n",
    "    test_vec = tfs.transform([test_sentence])\n",
    "    proba_arr = model.predict_proba(test_vec)\n",
    "    label = proba_arr.argmax()\n",
    "    weight = proba_arr.max()\n",
    "    print ('class: {0}, proba: {1:.5f}'.format(label2name[label], weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Задание 8\n",
    "![title](./data/img/task8_1.png)\n",
    "* Single-link\n",
    "* complete link\n",
    "\n",
    "Косинусная мера сходства\n",
    "\n",
    "Координаты точек:\n",
    "* a (0.6, 1.9)\n",
    "* b (1.8, 1.6)\n",
    "* c  (2.7, 2.0)\n",
    "* d (3.0, 2.1)\n",
    "* e (3.0, 2.6)\n",
    "* f (3.1, 4.5)\n",
    "* g (3.8, 0.6)\n",
    "* h (4.2, 2.7)\n",
    "\n",
    "Выполнить кластеризацию и показать на рисунке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "x = [0.6, 1.8, 2.7,3.0,3.0,3.1,3.8,4.2]\n",
    "y = [1.9, 1.6, 2.0, 2.1, 2.6, 4.5, 0.6,2.7]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "for i, label in enumerate(labels):\n",
    "    ax.annotate(label, (x[i],y[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "points = [(x,y) for x,y in zip(x,y)]\n",
    "array_cos = np.zeros((len(points), len(points)))\n",
    "for idx in range(len(labels)):\n",
    "    for idy in range(len(labels)):\n",
    "        if idx == idy:\n",
    "            array_cos[idx,idy] = 0.0\n",
    "        else:\n",
    "            array_cos[idx,idy] = 1-cosine_similarity(points[idx], points[idy])[0][0]\n",
    "cos_data = pd.DataFrame(array_cos, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sort(np.unique(array_cos.reshape(1,-1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-link\n",
    "#### Iteration 1\n",
    "|          |     a    |     b    |     c    |     d     |     e    |     f     |    g    |     h    |\n",
    "|----------|----------|----------|----------|-----------|----------|-----------|---------|----------|\n",
    "|    a     |----------|  0.1414  |  0.1904  |    0.2064 |   0.1479 |    0.0439 |   0.5538|  0.2310  |\n",
    "|    b     |----------|----------|   0.0039 |    0.0067 |  0.00008 | 0.0289    |  0.1581 |  0.0120  |\n",
    "|    c     |----------|----------|----------|    0.0003 |   0.0029 |0.053963   |0.113443 | 0.002191 |\n",
    "|    d     |----------|----------|----------|-----------| 0.005337 | 0.062995  |0.101354 |0.000776  |\n",
    "|    e     |----------|----------|----------|-----------|----------| 0.031953  |0.151414 |0.010172  |\n",
    "|    f     |----------|----------|----------|-----------|----------|-----------|0.311201 |0.077477  |\n",
    "|    g     |----------|----------|----------|-----------|----------|-----------|---------|0.084777  |\n",
    "\n",
    "#### Iteration 2\n",
    "|          |     a    |     b,e  |     c    |     d     |    f     |    g    |     h    |\n",
    "|----------|----------|----------|----------|-----------|----------|---------|----------|\n",
    "|    a     |----------|  0.1414  |  0.1904  |    0.2064 |   0.0439 |   0.5538|  0.2310  |\n",
    "|    b,e   |----------|----------|  0.002928|   0.005337|0.028882  | 0.151414| 0.010172 |\n",
    "|    c     |----------|----------|----------|    0.0003 |0.053963  |0.113443 | 0.002191 |\n",
    "|    d     |----------|----------|----------|-----------|0.062995  |0.101354 |0.000776  |\n",
    "|    f     |----------|----------|----------|-----------|----------|0.311201 |0.077477  |\n",
    "|    g     |----------|----------|----------|-----------|----------|---------|0.084777  |\n",
    "\n",
    "#### Iteration 3\n",
    "|          |     a    |     b,e  |     c,d  |    f     |    g    |     h    |\n",
    "|----------|----------|----------|----------|----------|---------|----------|\n",
    "|    a     |----------|  0.1414  |  0.1904  |   0.0439 |   0.5538|  0.2310  |\n",
    "|    b,e   |----------|----------|  0.002928|0.028882  | 0.151414| 0.010172 |\n",
    "|    c,d   |----------|----------|----------|0.053963  |0.101354 |0.000776  |\n",
    "|    f     |----------|----------|----------|----------|0.311201 |0.077477  |\n",
    "|    g     |----------|----------|----------|----------|---------|0.084777  |\n",
    "\n",
    "#### Iteration 4\n",
    "|          |     a    |     b,e  |   c,d,h  |    f     |    g    |\n",
    "|----------|----------|----------|----------|----------|---------|\n",
    "|    a     |----------|  0.1414  |  0.1904  |   0.0439 |   0.5538|\n",
    "|    b,e   |----------|----------|  0.002928|0.028882  | 0.151414|\n",
    "|    c,d,h |----------|----------|----------|0.053963  |0.084777 |\n",
    "|    f     |----------|----------|----------|----------|0.311201 |\n",
    "|    g     |----------|----------|----------|----------|---------|\n",
    "\n",
    "#### Iteration 5\n",
    "|          |     a    |b,e,c,d,h |    f     |    g    |\n",
    "|----------|----------|----------|----------|---------|\n",
    "|    a     |----------|  0.1904  |   0.0439 |   0.5538|\n",
    "|b,e,c,d,h |----------|----------|0.028882  |0.084777 |\n",
    "|    f     |----------|----------|----------|0.311201 |\n",
    "|    g     |----------|----------|----------|---------|\n",
    "\n",
    "#### Iteration 6\n",
    "|           |     a    |b,e,c,d,h,f|    g    |\n",
    "|-----------|----------|-----------|---------|\n",
    "|    a      |----------| 0.0439    |   0.5538|\n",
    "|b,e,c,d,h,f|----------|-----------|0.084777 |\n",
    "|    g      |----------|-----------|---------|\n",
    "\n",
    "#### Iteration 7\n",
    "|             |a,b,c,d,e,f,h|    g    |\n",
    "|-------------|-------------|---------|\n",
    "|a,b,c,d,e,f,h|-------------|0.084777 |\n",
    "|    g        |-------------|---------|\n",
    "\n",
    "![title](./data/img/single-link.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete-link\n",
    "#### Iteration 1\n",
    "|          |     a    |     b    |     c    |     d     |     e    |     f     |    g    |     h    |\n",
    "|----------|----------|----------|----------|-----------|----------|-----------|---------|----------|\n",
    "|    a     |----------|  0.1414  |  0.1904  |    0.2064 |   0.1479 |    0.0439 |   0.5538|  0.2310  |\n",
    "|    b     |----------|----------|   0.0039 |    0.0067 |  0.00008 | 0.0289    |  0.1581 |  0.0120  |\n",
    "|    c     |----------|----------|----------|    0.0003 |   0.0029 |0.053963   |0.113443 | 0.002191 |\n",
    "|    d     |----------|----------|----------|-----------| 0.005337 | 0.062995  |0.101354 |0.000776  |\n",
    "|    e     |----------|----------|----------|-----------|----------| 0.031953  |0.151414 |0.010172  |\n",
    "|    f     |----------|----------|----------|-----------|----------|-----------|0.311201 |0.077477  |\n",
    "|    g     |----------|----------|----------|-----------|----------|-----------|---------|0.084777  |\n",
    "\n",
    "#### Iteration 2\n",
    "|          |     a    |     b,e  |     c    |     d     |    f     |    g    |     h    |\n",
    "|----------|----------|----------|----------|-----------|----------|---------|----------|\n",
    "|    a     |----------|  0.1479  |  0.1904  |    0.2064 |   0.0439 |   0.5538|  0.2310  |\n",
    "|    b,e   |----------|----------| 0.0039   |   0.0067  |0.031953  | 0.1581  | 0.0120   |\n",
    "|    c     |----------|----------|----------|    0.0003 |0.053963  |0.113443 | 0.002191 |\n",
    "|    d     |----------|----------|----------|-----------|0.062995  |0.101354 |0.000776  |\n",
    "|    f     |----------|----------|----------|-----------|----------|0.311201 |0.077477  |\n",
    "|    g     |----------|----------|----------|-----------|----------|---------|0.084777  |\n",
    "\n",
    "#### Iteration 3\n",
    "|          |     a    |     b,e  |     c,d  |    f     |    g    |     h    |\n",
    "|----------|----------|----------|----------|----------|---------|----------|\n",
    "|    a     |----------|  0.1479  |  0.2064  |   0.0439 |   0.5538|  0.2310  |\n",
    "|    b,e   |----------|----------| 0.0067   |0.031953  | 0.1581  | 0.0120   |\n",
    "|    c,d   |----------|----------|----------|0.062995  |0.113443 | 0.002191 |\n",
    "|    f     |----------|----------|----------|----------|0.311201 |0.077477  |\n",
    "|    g     |----------|----------|----------|----------|---------|0.084777  |\n",
    "\n",
    "#### Iteration 4\n",
    "|          |     a    |     b,e  |   c,d,h  |    f     |    g    |\n",
    "|----------|----------|----------|----------|----------|---------|\n",
    "|    a     |----------|  0.1479  |  0.2310  |   0.0439 |   0.5538|\n",
    "|    b,e   |----------|----------| 0.0120   |0.031953  | 0.1581  |\n",
    "|    c,d,h |----------|----------|----------|0.077477  |0.113443 |\n",
    "|    f     |----------|----------|----------|----------|0.311201 |\n",
    "|    g     |----------|----------|----------|----------|---------|\n",
    "\n",
    "#### Iteration 5\n",
    "|          |     a    | b,e,c,d,h|    f     |    g    |\n",
    "|----------|----------|----------|----------|---------|\n",
    "|    a     |----------|  0.2310  |   0.0439 |   0.5538|\n",
    "|b,e,c,d,h |----------|----------|0.077477  | 0.1581  |\n",
    "|    f     |----------|----------|----------|0.311201 |\n",
    "|    g     |----------|----------|----------|---------|\n",
    "\n",
    "#### Iteration 6\n",
    "|          |     a,f  | b,e,c,d,h|    g    |\n",
    "|----------|----------|----------|---------|\n",
    "|    a,f   |----------|  0.2310  |   0.5538|\n",
    "|b,e,c,d,h |----------|----------| 0.1581  |\n",
    "|    g     |----------|----------|---------|\n",
    "\n",
    "#### Iteration 7\n",
    "|           |     a,f  |b,e,c,d,h,g|\n",
    "|-----------|----------|-----------|\n",
    "|    a,f    |----------|     0.5538|\n",
    "|b,e,c,d,h,g|----------|-----------|\n",
    "\n",
    "![title](./data/img/complete-link.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 9\n",
    "![title](./data/img/task9.png)\n",
    "Дан веб-граф, составить матрицу переходов. \n",
    "<br\\>\n",
    "Коэффициент телепортации = 0.1.\n",
    "<br\\>\n",
    "Составить матрицу переходов и вычислить pagerank для узлов сети\n",
    "<br \\>\n",
    "Начальный вектор состояний можно взять с равными вероятностями для каждого состояния"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d = 0.1$\n",
    "$(1 - d) = 0.9$\n",
    "<br />\n",
    "начальный вектор состояний:\n",
    "* $v = (0.33, 0.33, 0.33)$\n",
    "\n",
    "\n",
    "Составим матрицу переходов P:\n",
    "\n",
    "|    | d1 | d2 | d3 |\n",
    "|----|----|----|----|\n",
    "| d1 |  0 |  1 |  1 |\n",
    "| d2 |  0 |  0 |  1 |\n",
    "| d3 |  0 |  1 |  0 |\n",
    "\n",
    "\n",
    "Нормализуем строки:\n",
    "\n",
    "|    | d1 | d2 | d3 |\n",
    "|----|----|----|----|\n",
    "| d1 |  0 | 1/2| 1/2|\n",
    "| d2 |  0 |  0 |  1 |\n",
    "| d3 |  0 |  1 |  0 |\n",
    "\n",
    "\n",
    "Умножаем на (1-d):\n",
    "\n",
    "|    | d1 | d2 | d3 |\n",
    "|----|----|----|----|\n",
    "| d1 |  0 |0.45|0.45|\n",
    "| d2 |  0 |  0 | 0.9|\n",
    "| d3 |  0 |0.9 |  0 |\n",
    "\n",
    "\n",
    "К каждой строке добавляем 0.1/3 = 1/30 = 0.033\n",
    "\n",
    "|    |  d1  |  d2  |  d3  |\n",
    "|----|------|------|------|\n",
    "| d1 |0.033 |0.483 |0.483 | \n",
    "| d2 |0.033 |0.033 |0.933 |\n",
    "| d3 | 0.033|0.933 | 0.033|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = np.array([[0.033,0.483,0.483], [0.033,0.033,0.933],[0.033,0.933,0.033]])\n",
    "#Посчитаем vP, vP^2 и тд:\n",
    "newP = P\n",
    "#10 раз\n",
    "for _ in range(10):\n",
    "    newP = newP.dot(P)\n",
    "print (newP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pageRank(d1) == 0.033\n",
    "<br/>\n",
    "pageRank(d2) == 0.478\n",
    "<br/>\n",
    "pageRank(d3) == 0.478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
